{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_cs2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnSmHU65Nky"
      },
      "source": [
        "import os\r\n",
        "os.chdir('drive/My Drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvEFyQeT7PST"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.compat.v1.enable_eager_execution()\r\n",
        "from tensorflow.keras.layers import TimeDistributed\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ4UhcYw6v9k"
      },
      "source": [
        "infile = open('tokenizers_subject.pkl','rb')\r\n",
        "tokenizer_encoder, tokenizer_decoder = pickle.load(infile)\r\n",
        "infile.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAZmXFXr7bCp"
      },
      "source": [
        "def custom_lossfunction(targets,logits):\r\n",
        "\r\n",
        "  # Custom loss function that will not consider the loss for padded zeros.\r\n",
        "  # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\r\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\r\n",
        "  loss_ = loss_object(targets, logits)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "  loss_ *= mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss_)\r\n",
        "\r\n",
        "tf.keras.losses.custom_loss = custom_lossfunction"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3NKz8Bk65tr"
      },
      "source": [
        "loaded_model_att = tf.keras.models.load_model('final_attention_subject_upd',custom_objects={'custom_lossfunction':custom_lossfunction})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36nEsrwK7tEF"
      },
      "source": [
        "model = loaded_model_att"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhaAEe0g7LEG"
      },
      "source": [
        "import copy\r\n",
        "def final_fun_1(input_sentence):\r\n",
        "\r\n",
        "  '''\r\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\r\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\r\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to decoder\r\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\r\n",
        "         predicted_out,state_h,state_c=model.layers[1](dec_input,states)\r\n",
        "         pass the predicted_out to the dense layer\r\n",
        "         update the states=[state_h,state_c]\r\n",
        "         And get the index of the word with maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\r\n",
        "         Update the input_to_decoder with current predictions\r\n",
        "  F. Return the predicted sentence\r\n",
        "  '''\r\n",
        "  encoder_test_tokens = tokenizer_encoder.texts_to_sequences([input_sentence])\r\n",
        "  padded_encoder_input = pad_sequences(encoder_test_tokens, maxlen=16, dtype='float32', padding='post')\r\n",
        "  encoder = model.layers[2]\r\n",
        "  encoder_op, enc_h, enc_c = encoder(padded_encoder_input)\r\n",
        "  decoder = model.layers[4]\r\n",
        "  index_of_start = np.array(tokenizer_decoder.word_index['<start>']).reshape(1,1).astype('float32')\r\n",
        "  predicted_out,enc_h, enc_c,attention,context_vector = decoder.onestepdecoder(index_of_start,encoder_op, enc_h, enc_c)\r\n",
        "  state_h, state_c = enc_h,enc_c\r\n",
        "  states = (state_h, state_c)\r\n",
        "  toppred = np.argsort(predicted_out[0])[-3:][::-1]\r\n",
        "  probs = np.sort(predicted_out[0])[-3:][::-1]\r\n",
        "  words = []\r\n",
        "  for pred in toppred:\r\n",
        "    word = [k for k in tokenizer_decoder.word_index if tokenizer_decoder.word_index[k]==pred][0]\r\n",
        "    words.append(word)\r\n",
        "  semi_final = [[probs[0],[toppred[0]],[words[0]],states],[probs[1],[toppred[1]],[words[1]],states],[probs[2],[toppred[2]],[words[2]],states]]\r\n",
        "  finished_sentences = 0\r\n",
        "  final = []\r\n",
        "  while (True):\r\n",
        "    temp = []\r\n",
        "    for i in range(len(semi_final)):\r\n",
        "      # dec_emb= decoder.embedding(semi_final[i][1][-1].reshape(1,1))\r\n",
        "      predicted_out,state_h, state_c,attention,context_vector = decoder.onestepdecoder(semi_final[i][1][-1].reshape(1,1).astype('float32'),\r\n",
        "                                                                                       encoder_op, semi_final[i][-1][0], semi_final[i][-1][1])\r\n",
        "      toppred = np.argsort(predicted_out[0])[-len(semi_final):][::-1]\r\n",
        "      probs = np.sort(predicted_out[0])[-len(semi_final):][::-1]\r\n",
        "      states= (state_h, state_c)\r\n",
        "      for j in range(len(toppred)):\r\n",
        "        word = [k for k in tokenizer_decoder.word_index if tokenizer_decoder.word_index[k]==toppred[j]][0]\r\n",
        "        #temp[str(i)+','+str(j)] = (semi_final[i][0] * probs[j],toppred[j],semi_final[i][2].append(word),states)\r\n",
        "        words = copy.deepcopy(semi_final[i][2])\r\n",
        "        words.append(word)\r\n",
        "        temp.append([semi_final[i][0] * probs[j],[toppred[j]],words,states])\r\n",
        "    temp = sorted(temp,key = lambda x:x[0],reverse=True)[:len(semi_final)]\r\n",
        "    ids_to_be_removed = []\r\n",
        "    for id,k in enumerate(temp):\r\n",
        "      if k[2][-1] == '<end>':\r\n",
        "        final.append((k[0],' '.join(k[2][:-1])))\r\n",
        "        finished_sentences+=1\r\n",
        "        ids_to_be_removed.append(id)\r\n",
        "    for id in ids_to_be_removed:\r\n",
        "      temp[id] = 0\r\n",
        "    temp = [i for i in temp if i!=0]\r\n",
        "    semi_final=temp\r\n",
        "    if finished_sentences == 3:\r\n",
        "      break\r\n",
        "  predictions_3 = [x[1] for x in final]\r\n",
        "  return predictions_3"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F7oLc2w73UE",
        "outputId": "82f0ca3c-23a4-474a-e5a8-6a1e2864bf08"
      },
      "source": [
        "final_fun_1(\"i have sent you\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a copy of', 'a copy in', 'a confirmation of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7lZ5j8f8KEc"
      },
      "source": [
        "def predict(input_sentence):\r\n",
        "\r\n",
        "  '''\r\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\r\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\r\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\r\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\r\n",
        "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\r\n",
        "         Save the attention weights\r\n",
        "         And get the word using the tokenizer(word index) and then store it in a string.\r\n",
        "  E. Call plot_attention(#params)\r\n",
        "  F. Return the predicted sentence\r\n",
        "  ''' \r\n",
        "  encoder_test_tokens = tokenizer_encoder.texts_to_sequences([input_sentence])\r\n",
        "  padded_encoder_input = pad_sequences(encoder_test_tokens, maxlen=16, dtype='float32', padding='post')\r\n",
        "  encoder = model.layers[2]\r\n",
        "  encoder_op, enc_h, enc_c = encoder(padded_encoder_input)\r\n",
        "  decoder = model.layers[4]\r\n",
        "  index_of_start = np.array(tokenizer_decoder.word_index['<start>']).reshape(1,1).astype('float32')\r\n",
        "  pred=0\r\n",
        "  sentence = []\r\n",
        "  attention_weights=[]\r\n",
        "  # att_wgts = tf.TensorArray(dtype=tf.float32, dynamic_size=True,size=0)\r\n",
        "  while pred!=tokenizer_decoder.word_index['<end>']:\r\n",
        "    predicted_out,enc_h, enc_c,attention,context_vector = decoder.onestepdecoder(index_of_start,encoder_op, enc_h, enc_c)\r\n",
        "    # att_wgts = att_wgts.write(att_wgts.size(),tf.reshape(attention,(14,)))\r\n",
        "    pred = np.argmax(predicted_out) \r\n",
        "    word = [k for k in tokenizer_decoder.word_index if tokenizer_decoder.word_index[k]==(pred)][0]\r\n",
        "    sentence.append(word)\r\n",
        "    index_of_start = np.array(pred).reshape(1,1).astype('float32')\r\n",
        "\r\n",
        "  return ' '.join(sentence[:-1])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7XEVsvL-Xnj"
      },
      "source": [
        "import pickle\r\n",
        "infile = open('final_data_cs2_subject.pkl','rb')\r\n",
        "new_data = pickle.load(infile)\r\n",
        "infile.close()\r\n",
        "\r\n",
        "new_data['decoder_input'] = '<start> ' + new_data['output'].astype(str)\r\n",
        "new_data['decoder_output'] = new_data['output'].astype(str) + ' <end>'\r\n",
        "new_data = new_data.drop(['output'], axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jsg_3o4-F6z"
      },
      "source": [
        "sample = new_data.sample(1000)\r\n",
        "import nltk.translate.bleu_score as bleu"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Te6tCl-qhq"
      },
      "source": [
        "def final_fun_2(X,y):\r\n",
        "  blue_scores=[]\r\n",
        "  for i in X.index:\r\n",
        "    predicted = predict(X[i].lower())\r\n",
        "    predicted = predicted.split()\r\n",
        "    original = [x for x in y[i].split() if x!='<start>']\r\n",
        "    blue_scores.append(bleu.sentence_bleu([original],predicted))\r\n",
        "\r\n",
        "  return np.mean(blue_scores)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddh2TzLn_t9e",
        "outputId": "00ddbf2a-999b-46ee-c417-f150ca1d03c3"
      },
      "source": [
        "final_fun_2(sample.input,sample.decoder_input)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7040402629024597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKJ1ppR__4EZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}